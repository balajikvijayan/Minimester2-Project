{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from mlxtend.classifier import EnsembleClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cross_validation import KFold, train_test_split,\\\n",
    "                    cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "                    f1_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier,\\\n",
    "                            RandomForestClassifier\n",
    "from pymongo import MongoClient\n",
    "from code.util import Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "#Lists for X and y\n",
    "review_list, opinion_list, sentiword_list, sentiment_list, word_list,\\\n",
    "            pos, neg = [],[],[],[],[],[],[]\n",
    "vocab = {}\n",
    "    \n",
    "#PyMongo variables\n",
    "client = MongoClient()\n",
    "db = client['reviews']\n",
    "collection = db['movies']\n",
    "reviews = collection.find()\n",
    "\n",
    "db1 = client['sentiment']\n",
    "collection1 = db1['bingliu']\n",
    "sentiments = collection1.find()\n",
    "\n",
    "#build review and label lists\n",
    "for review in reviews:\n",
    "    opinion_list.append(review['Opinion'])\n",
    "    review_list.append(review['Review'])\n",
    "\n",
    "opinion_array = np.array(opinion_list)    \n",
    "\n",
    "#build sentiment word, sentiment polarity, pos word, neg word\n",
    "for sentiment in sentiments:\n",
    "    sentiword_list.append(sentiment['Word'])\n",
    "    sentiment_list.append(sentiment['Sentiment'])\n",
    "    if sentiment['Sentiment'] == 1:\n",
    "        pos.append(sentiment['Word'])\n",
    "    elif sentiment['Sentiment'] == -1:\n",
    "        neg.append(sentiment['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='replace',strip_accents='unicode',\\\n",
    "                            vocabulary = sentiword_list, lowercase=True)\n",
    "review_tfidf = vectorizer.fit_transform(review_list)\n",
    "review_sf = review_tfidf.copy()\n",
    "\n",
    "#for every review\n",
    "for i, review_s in enumerate(review_tfidf):\n",
    "    #for every index (word) in the review\n",
    "    for idx in review_tfidf[i].indices:\n",
    "        if vectorizer.vocabulary[idx] in neg:\n",
    "            review_sf[i, idx] = review_tfidf[i, idx]*-1\n",
    "        elif vectorizer.vocabulary[idx] in pos:\n",
    "            review_sf[i, idx] = review_tfidf[i, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.837 (std: 0.010)\n",
      "Parameters: {'penalty': 'l2', 'C': 1, 'fit_intercept': True, 'class_weight': 'auto'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.837 (std: 0.010)\n",
      "Parameters: {'penalty': 'l2', 'C': 1, 'fit_intercept': True, 'class_weight': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.837 (std: 0.011)\n",
      "Parameters: {'penalty': 'l2', 'C': 1, 'fit_intercept': False, 'class_weight': 'auto'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "param_dist = {\"penalty\": ['l1', 'l2'],\n",
    "              \"class_weight\": [\"auto\",None],\n",
    "              \"C\": [1,.1,.01],\n",
    "              \"fit_intercept\": [True,False]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 24\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='accuracy')\n",
    "random_search.fit(review_sf, opinion_array)\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.818 (std: 0.012)\n",
      "Parameters: {'max_features': 'auto', 'n_estimators': 100, 'criterion': 'entropy', 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.814 (std: 0.003)\n",
      "Parameters: {'max_features': 'sqrt', 'n_estimators': 100, 'criterion': 'entropy', 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.810 (std: 0.009)\n",
      "Parameters: {'max_features': 'sqrt', 'n_estimators': 100, 'criterion': 'gini', 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\"n_estimators\": [10, 100],\n",
    "              \"criterion\": [\"gini\",\"entropy\"],\n",
    "              \"max_features\": [\"auto\",\"sqrt\",\"log2\"],\n",
    "              \"max_depth\": [None,5,10]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 36\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='accuracy')\n",
    "random_search.fit(review_sf, opinion_array)\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.724 (std: 0.025)\n",
      "Parameters: {'alpha': 1, 'fit_prior': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.724 (std: 0.025)\n",
      "Parameters: {'alpha': 1, 'fit_prior': False}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.712 (std: 0.024)\n",
      "Parameters: {'alpha': 0.1, 'fit_prior': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utility function to report best scores\n",
    "clf = BernoulliNB()\n",
    "\n",
    "param_dist = {\"alpha\": [0,.1,1],\n",
    "              \"fit_prior\": [True,False]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 6\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='accuracy')\n",
    "random_search.fit(review_sf, opinion_array)\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84 (+/- 0.01) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression('l2')\n",
    "clf2 = RandomForestClassifier(n_estimators=100,criterion='entropy')\n",
    "clf3 = BernoulliNB(alpha=1,fit_prior=True)\n",
    "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n",
    "for clf, label in zip([eclf], ['Ensemble']):\n",
    "    scores = cross_val_score(clf, review_sf, opinion_array,\\\n",
    "                cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_topics = 1\n",
    "n_top_words = 1000\n",
    "\n",
    "neg_vectorizer = TfidfVectorizer(decode_error='replace',strip_accents='unicode',\\\n",
    "                            vocabulary = neg, lowercase=True)\n",
    "neg_tfidf = neg_vectorizer.fit_transform(review_list[0:1000])\n",
    "pos_vectorizer = TfidfVectorizer(decode_error='replace',strip_accents='unicode',\\\n",
    "                            vocabulary = pos, lowercase=True)\n",
    "pos_tfidf = pos_vectorizer.fit_transform(review_list[1000:2000])\n",
    "\n",
    "neg_nmf = NMF(n_components=n_topics, random_state=1).fit(neg_tfidf)\n",
    "pos_nmf = NMF(n_components=n_topics, random_state=1).fit(pos_tfidf)\n",
    "\n",
    "neg_feature_names = neg_vectorizer.get_feature_names()\n",
    "pos_feature_names = pos_vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(pos_nmf.components_):\n",
    "    pos_nmf = [pos_feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "\n",
    "for topic_idx, topic in enumerate(neg_nmf.components_):\n",
    "    neg_nmf = [neg_feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.831 (std: 0.009)\n",
      "Parameters: {'penalty': 'l2', 'C': 1, 'fit_intercept': True, 'class_weight': 'auto'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.831 (std: 0.009)\n",
      "Parameters: {'penalty': 'l2', 'C': 1, 'fit_intercept': True, 'class_weight': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.829 (std: 0.012)\n",
      "Parameters: {'penalty': 'l2', 'C': 1, 'fit_intercept': False, 'class_weight': 'auto'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmfvectorizer = TfidfVectorizer(decode_error='replace',strip_accents='unicode',\\\n",
    "                    vocabulary = pos_nmf+neg_nmf, lowercase=True)\n",
    "review_tfidf = nmfvectorizer.fit_transform(review_list)\n",
    "review_sf = review_tfidf.copy()\n",
    "\n",
    "#for every review\n",
    "for i, review_s in enumerate(review_tfidf):\n",
    "    #for every index (word) in the review\n",
    "    for idx in review_tfidf[i].indices:\n",
    "        if nmfvectorizer.vocabulary[idx] in neg_nmf:\n",
    "            review_sf[i, idx] = review_tfidf[i, idx]*-1\n",
    "        elif nmfvectorizer.vocabulary[idx] in pos:\n",
    "            review_sf[i, idx] = review_tfidf[i, idx]\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "param_dist = {\"penalty\": ['l1', 'l2'],\n",
    "              \"class_weight\": [\"auto\",None],\n",
    "              \"C\": [1,.1,.01],\n",
    "              \"fit_intercept\": [True,False]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 24\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='accuracy')\n",
    "random_search.fit(review_sf, opinion_array)\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.821 (std: 0.017)\n",
      "Parameters: {'max_features': 'log2', 'n_estimators': 100, 'criterion': 'entropy', 'max_depth': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.820 (std: 0.020)\n",
      "Parameters: {'max_features': 'log2', 'n_estimators': 100, 'criterion': 'gini', 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.813 (std: 0.013)\n",
      "Parameters: {'max_features': 'auto', 'n_estimators': 100, 'criterion': 'gini', 'max_depth': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\"n_estimators\": [10, 100],\n",
    "              \"criterion\": [\"gini\",\"entropy\"],\n",
    "              \"max_features\": [\"auto\",\"sqrt\",\"log2\"],\n",
    "              \"max_depth\": [None,5,10]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 36\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='accuracy')\n",
    "random_search.fit(review_sf, opinion_array)\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.722 (std: 0.020)\n",
      "Parameters: {'alpha': 1, 'fit_prior': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.722 (std: 0.020)\n",
      "Parameters: {'alpha': 1, 'fit_prior': False}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.712 (std: 0.022)\n",
      "Parameters: {'alpha': 0.1, 'fit_prior': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utility function to report best scores\n",
    "clf = BernoulliNB()\n",
    "\n",
    "param_dist = {\"alpha\": [0,.1,1],\n",
    "              \"fit_prior\": [True,False]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 6\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='accuracy')\n",
    "random_search.fit(review_sf, opinion_array)\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84 (+/- 0.02) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression('l2')\n",
    "clf2 = RandomForestClassifier(max_features='log2',n_estimators=100,\\\n",
    "                              criterion='entropy')\n",
    "clf3 = BernoulliNB(alpha=0,fit_prior=True)\n",
    "eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1])\n",
    "for clf, label in zip([eclf], ['Ensemble']):\n",
    "    scores = cross_val_score(clf, review_sf, opinion_array,\\\n",
    "                cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
