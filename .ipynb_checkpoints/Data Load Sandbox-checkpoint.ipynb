{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "#Path of data files\n",
    "pospath = r'C:\\Anaconda\\Galvanize\\Minimester2-Project\\data\\txt_sentoken\\pos'\n",
    "negpath = r'C:\\Anaconda\\Galvanize\\Minimester2-Project\\data\\txt_sentoken\\neg'\n",
    "\n",
    "#Other initializations including MongoDB and lists of data\n",
    "client = MongoClient()\n",
    "db = client.reviews\n",
    "collection = db.movies\n",
    "review = []\n",
    "opinion = []\n",
    "\n",
    "#Drop MongoDB data so duplicates aren't loaded\n",
    "collection.drop()\n",
    "\n",
    "#loop through neg files\n",
    "for name in os.listdir(negpath):\n",
    "    #context manager so files aren't floating around\n",
    "    with open(negpath+'\\\\'+name, 'r') as fo:\n",
    "        #read in the data\n",
    "        data = fo.read()\n",
    "        #remove apostrophes so that contractions are treated as one word\n",
    "        #otherwise wouldn't, can't, won't etc will all tokenize to t\n",
    "        review.append(data.replace(\"'\",\"\"))\n",
    "        opinion.append(False)\n",
    "\n",
    "#loop through pos files\n",
    "for name in os.listdir(pospath):\n",
    "    #context manager so files aren't floating around\n",
    "    with open(pospath+'\\\\'+name, 'r') as fo:\n",
    "        #read in the data\n",
    "        data = fo.read()\n",
    "        review.append(data.replace(\"'\",\"\"))\n",
    "        opinion.append(True)\n",
    "\n",
    "df = pd.DataFrame(zip(review,opinion))\n",
    "df.columns = ['Review','Opinion']\n",
    "df.head()\n",
    "result = collection.insert(df.T.to_dict().values())\n",
    "print collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6784 Records loaded\n"
     ]
    }
   ],
   "source": [
    "sentimentpath = 'C:\\Anaconda\\Galvanize\\Minimester2-Project\\data\\sentiment'\n",
    "#Path of data files\n",
    "pospath, negpath = sentimentpath+r'\\positive-words.txt', sentimentpath+r'\\negative-words.txt'\n",
    "#PyMongo variables\n",
    "client = MongoClient()\n",
    "db = client['sentiment']\n",
    "collection = db['bingliu']\n",
    "#Data lists\n",
    "words, sentiment, pos, neg = [],[],[],[]\n",
    "#Drop the existing MongoDB data so duplicates aren't loaded\n",
    "collection.drop()\n",
    "#loop through neg files\n",
    "for name in [pospath, negpath]:\n",
    "    #context manager so files aren't floating around\n",
    "    with open(name, 'r') as fo:\n",
    "        #read in the data\n",
    "        data = fo.read()\n",
    "        u_data = data.decode('cp1252') \n",
    "        utf8_data = u_data.encode(\"utf8\").split('\\n')        \n",
    "        #Label!\n",
    "        if name == pospath:\n",
    "            pos.extend(utf8_data)\n",
    "        if name == negpath:\n",
    "            neg.extend(utf8_data)\n",
    "\n",
    "for word in list(set(pos) & set(neg)):\n",
    "    #de-dupe words in both pos and neg lexicon\n",
    "    del pos[pos.index(word)]\n",
    "    del neg[neg.index(word)]\n",
    "\n",
    "#now we build the proper sentiment and word lists to load\n",
    "sentiment.extend([1]*len(pos))\n",
    "sentiment.extend([-1]*len(neg))\n",
    "words.extend(pos)\n",
    "words.extend(neg)\n",
    "\n",
    "df = pd.DataFrame(zip(words,sentiment))\n",
    "df.columns = ['Word','Sentiment']\n",
    "result = collection.insert(df.T.to_dict().values())\n",
    "print \"{} Records loaded\".format(collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8221 Records loaded\n"
     ]
    }
   ],
   "source": [
    "#Path of data file\n",
    "path = 'C:\\Anaconda\\Galvanize\\Minimester2-Project\\data\\sentiment\\subjclueslen1-HLTEMNLP05.tff'\n",
    "#PyMongo variables\n",
    "client = MongoClient()\n",
    "db = client['sentiment']\n",
    "collection = db['mpqa']\n",
    "collection.drop()\n",
    "sentiment_df = pd.read_csv(path, sep=' ')\n",
    "sentiment_df.columns = ['Type','Len', 'Word1', 'Pos1','Stemmed1',\\\n",
    "                        'PriorPolarity']\n",
    "sentiment_df.iloc[:, 0] = sentiment_df.iloc[:, 0].str.replace('type=', '')\n",
    "sentiment_df.iloc[:, 1] = sentiment_df.iloc[:, 1].str.replace('len=', '')\n",
    "sentiment_df.iloc[:, 2] = sentiment_df.iloc[:, 2].str.replace('word1=', '')\n",
    "sentiment_df.iloc[:, 3] = sentiment_df.iloc[:, 3].str.replace('pos1=', '')\n",
    "sentiment_df.iloc[:, 4] = sentiment_df.iloc[:, 4].str.replace('stemmed1=', '')\n",
    "sentiment_df.iloc[:, 5] = sentiment_df.iloc[:, 5].str.replace('priorpolarity=', '')\n",
    "\n",
    "result = collection.insert(sentiment_df.T.to_dict().values())\n",
    "print \"{} Records loaded\".format(collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
