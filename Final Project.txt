Models: Logistic Regression, Bernoulli Naive Bayes, Ensemble of Logit & BernoulliNB, NMF->Logit, NMF->BernoulliNB, NMF->Ensemble of Logit & BernoulliNB
See LogisticRegression.py
See BernoulliNB.py
See NMF.py

Tokenization: Standard tokenization using spaces, removed all apostrophes so that the part of the contraction after the apostrophe is not treated as its own word. (e.g. don't -> dont instead of don't -> don + t) This is done so that the single letter tail of a contraction aren't grouped with other single letter tails of contractions. There were no minimum or maximum values for terms to appear in reviews for the term to be included as a feature. The only words that were removed were English stop words as those have a polarity approximately equal to 0. The vocabulary used for tokenization was either Bing Liu's sentiment lexicons or words generated from an NMF model on the review data. When words were generated from the NMF model the subsequent classification model was tested using varying sizes of the list of words output from the NMF model.

No tokenization of sentences and N-grams; reason was the marginal increase in scoring in exchange for model complexity and feature engineering. In a scenario such as sentiment analysis on topics within a sentence (as opposed to the overall review) these features would be considered as part of feature selection/engineering.

For Non-NMF models, the approach I took was to use a TFIDF value for each term in every review that appears in Bing Liu's lexicon (his lexicon = tfidf vocabulary) and assigned positive values for the positive lexicons and negative vaues for the negative lexicons. This matrix was set as the X data and the known labels were used as the Y labels and used to fit and score my models (Logistic Regression, Bernoulli NB, Ensemble of the 2).

For NMF models, the approach I took was similar but instead of Bing Liu's lexicons I used NMF's property of latent topic modelling for sentiment analysis (n topics = 2; the topics roughly break down to positive & negative) to generate positive and negative lists of words. These lists of words were used to generate a TFIDF value for each term in every review (pos + neg lists = tfidf vocabulary) and assigned positive values for the positive lexicons and negative vaues for the negative lexicons. This matrix was set as the X data and the known labels were used as the Y labels and used to fit and score my models (Logistic Regression, Bernoulli NB, Ensemble of the 2).

Validation: For Non-NMF models I used the built in Cross Validation Score method in SKLearn. This splits the X data and the Y labels into K folds (parameter defined by the user) and calculates a scoring metric for each train/test fold. I then averaged the scores across K folds for an average score. This method was done to prevent any test/train split biases from using the same portion of data to either train or test my models and instead to smooth the test & train segments across all of the data and provide a model that is not overfit to the training data.

For NMF models I used all of the reviews to generate the positive and negative lists of words. From there I tried using varying sizes of these lists of words [10,20,50,100,1000,2000,5000] and then I used the built in Cross Validation Score method in SKLearn. This splits the X data and the Y labels into K folds and calculates a scoring metric for each train/test fold. I then averaged the scores across K folds for an average score. This method was done to prevent any test/train split biases from using the same portion of data to either train or test my models and instead to smooth the test & train segments across all of the data. Scoring was observed to increase as the number of words increased.

Normalization: No as I did not use the more advanced MPQA lexicons which can take advantage of stemming-based feature engineering. In addition lemmatization was not done for similar reasons; the required feature engineering would likely improve scoring at the expense of time and effort. In a scenario where higher accuracy and/or a scale of sentiment (0-5 or 0-10) is desired I'd likely switch from Bing Liu's lexicons to the MPQA lexicons and attempt additional feature engineering which would include stemming, lemmatization, POS tagging, word negation and several other NLP techniques.

Scoring: Weighted F1 Score and ROC_AUC. Weighted F1 Score so that we are able to equally weight precision and recall as well as each of the predicted classes. In this instance the predicted classes are equally sized but this implementation will be useful for scenarios with unbalanced class sizes. ROC_AUC is useful for determining the likelihood that the classifier will rank a random positively classified observation higher than a random negatively classified observation. ROC_AUC also has the benefits of capturing the FPR which is not included within the Weighted F1 Score. As this is a classification task we may want to heavily penalize one or more of the 4 quadrants of the confusion matrix (TP, FP, FN, TN), depending on which quadrants are most important to us will determine which metrics are useful for evaluating your model.

Tagging: No tags as a simple Logistic Regression with TFIDF of sentiment lexicons with simple leixcal scoring (pos=1,neg=-1) achieves a weighted f1 score ~.83. Were I to use the MPQA sentiment lexicons instead of Bing Liu's sentiment lexicons I would include POS tagging to better match the tokens in each review to their sentiment values.

Ensemble: I tried a technique of ensembling multiple classifiers to reduce the overall error and improve the overall score. Since this took the highest performing model as an input (Logistic Regression) the scoring of this ensemble never exceed the single highest performing model. However results were observed when poorer models were ensembled with the resultant performing better than any of the constituent components.

In addition I did an ensemble using an NMF of the reviews to generate positive and negative vocabulary that were then used by Logistic Regression, Bernoulli NB and the aforementioned ensemble classifier. In this exact use case I probably wouldn't use NMF for sentiment analysis. However it does have use cases in unsupervised and semi-supervised learning as it does not need the predefined lexicons and can instead attempt to discover these latent lexicon vocabularies. The scoring of models built using these latent lexicon vocabularies approached the score of models built using Bing Liu's lexicon once the size of the latent lexicon vocabularies grew large. (<2000 words) However this approach will overfit for words that appear in the reviews that the NMF model was trained on; given a larger sized review set it would be ideal to conduct cross validation on the NMF to prevent this overfitting.

Vectorizing: Term Frequency with IDF scaling as repeated words should have values that increase on a decreasing scale. The term frequencies are multiplied by -1 for negative words and 1 for positive words; words are assigned sentiment from either Bing Liu's lexicon or the latent lexicon vocabularies. All other words are not included in the vocabulary and thus are not considered as a feature for the purposes of sentiment analysis